---
title: "Computer Assignment 2"
author: "Gabriel Lindqvist, Jiahui Li"
date: "2024-11-24"
output: pdf_document
---
# Exercise2:1

Data on periodontitis (severe infammation of the gums) were collected from a group of adult patients at a large dental clinic.The patients were compared with a group of adult people with normal gums who visited the same clinic.The individuals in both groups were interviewed about the use of dental floss in the last five years,with the following result:
```{r}
tab1<- as.table(rbind(c(22, 75), c(148, 265)))
dimnames(tab1) <- list("Regularly use of dental floss" = c("Yes", "No"),Periodontitis = c("Yes","No"))
addmargins(tab1)
addmargins(prop.table(tab1,1),2)
```
## Task1
The problem asks us to analyze the relationship between regular dental floss use and the occurrence of periodontitis (gum disease) using a logistic regression model. The data provided include counts of individuals categorized by whether they regularly use dental floss (Yes/No) and whether they have periodontitis (Yes/No). The logistic regression model is specified as:

\[
\text{logit}(p_x) = \beta_0 + \beta_1 x
\]

Where:
- \(x = 1\) if the individual regularly uses dental floss, \(x = 0\) otherwise.
- \(p_x = P(\text{periodontitis} \mid x)\), the probability of having periodontitis given the flossing status.

The task is to:
1. Estimate the parameters \(\beta_0\) and \(\beta_1\).
2. Interpret these parameters in the context of the data.

```{r}
# Convert the table into a dataset for logistic regression
use<-c("no","no", "yes","yes")
per<-c("no","yes","no","yes")
n<-c(265,148,75,22)
data21 <- data.frame(use,per,n)
data21
```

```{r}
#Fit a logistic regression model
data21$use <- as.factor(data21$use)
data21$per <- as.factor(data21$per)
model21<-glm(per~use, weights =n, family = binomial(link=logit), data=data21)
summary(model21)
```
#### 1.From the output, we obtain:
- Intercept: \(\beta_0 = -0.5825\)
- Slope: \(\beta_1 = -0.6439\) (effect of `useyes` relative to `useno`)

Substituting these into the model:
\[
\text{logit}(p_x) = -0.5825 - 0.6439 \cdot x
\]

#### 2. Interpretation of the Parameters
- **Intercept (\(\beta_0 = -0.5825\))**:  
  When \(x = 0\) (no regular floss use), the log-odds of having periodontitis is \(-0.5825\). Converting this to a probability:
  \[
  P(\text{Periodontitis} \mid \text{use = no}) = \frac{\exp(-0.5825)}{1 + \exp(-0.5825)} \approx 0.358
  \]
  **Interpretation**: For individuals who do not floss regularly, the probability of having periodontitis is approximately 35.8%.

- **Slope (\(\beta_1 = -0.6439\))**:  
  When \(x = 1\) (regular floss use), the log-odds of periodontitis decreases by 0.6439. Converting this to an odds ratio (OR):
  \[
  \text{OR} = \exp(-0.6439) \approx 0.525
  \]
  **Interpretation**: Individuals who floss regularly are 52.5% as likely to have periodontitis compared to those who do not floss regularly. This indicates that regular flossing reduces the odds of periodontitis by approximately 47.5%.

#### 3. Statistical Significance of the Parameters
- **Intercept (\(\beta_0\))**: The \(p\)-value (\(< 0.001\)) shows that the baseline odds of periodontitis for individuals who do not floss regularly is statistically significant.
- **Slope (\(\beta_1\))**: The \(p\)-value (\(0.0145\)) indicates that the effect of flossing on the odds of periodontitis is statistically significant at the 5% level.

#### 4. Summary of Results
- **Regular floss use significantly reduces the likelihood of periodontitis**: The negative slope (\(\beta_1\)) and odds ratio (\(0.525\)) suggest that regular flossing is associated with a significant reduction in the odds of periodontitis by approximately 47.5%.
- **Baseline risk for non-floss users**: Individuals who do not floss regularly have a periodontitis probability of 35.8%.

In conclusion, regular flossing has a statistically significant protective effect against periodontitis, as shown by the model (\(p = 0.0145\)).


## Task2
This question asks us to fit a logistic regression model for the probability of **using dental floss** as a function of periodontitis status. The logistic regression model is specified as:

\[
\text{logit}(p_y) = \gamma_0 + \gamma_1 y
\]

Where:
- \(y = 1\) if the individual has periodontitis, and \(y = 0\) otherwise.
- \(p_y = P(\text{using dental floss} \mid y)\), the probability of using dental floss given the periodontitis status.

```{r}
# Fit a logistic regression model with periodontitis status as the predictor
model22 <- glm(use ~ per, weights = n, family = binomial(link = logit), data = data21)

# Display the summary of the model
summary(model22)
```
#### 1.From the output, we obtain:
- Intercept: \(\gamma_0 = -1.2622\)
- Slope: \(\gamma_1 = -0.6439\) (effect of `peryes` relative to `perno`)

Substituting these into the model:
\[
\text{logit}(p_x) = -1.2622 - 0.6439 \cdot x
\]

#### 2. Interpretation of the Parameters
- **Intercept (\(\gamma_0 = -1.2622\))**:  
  When \(y = 0\) (no periodontitis), the log-odds of regular floss use is \(-1.2622\).   Converting this to a probability:
  \[
  P(\text{using floss} \mid y = 0) = \frac{\exp(-1.2622)}{1 + \exp(-1.2622)} \approx 0.22
  \]
  **Interpretation**: Individuals without periodontitis have approximately a **22% probability of using dental floss**.

- **Slope (\(\gamma_1 = -0.6439\))**
  When \(y = 1\) (periodontitis is present), the log-odds of using dental floss decreases by 0.6439.
Converting this to an **odds ratio (OR)**:
  \[
  \text{OR} = \exp(-0.6439) \approx 0.525
  \]
  **Interpretation**: Individuals with periodontitis are **52.5% as likely** to use dental floss compared to those without periodontitis. In other words, the presence of periodontitis is associated with a lower likelihood of using dental floss.

#### 3. Statistical Significance of the Parameters
- **Intercept (\(\gamma_0\))**:
  - \(p < 0.001\): The intercept is significant, indicating that the baseline odds of using dental floss for individuals without periodontitis is significantly below 50%.
- **Slope (\(\gamma_1\))**:
  - \(p = 0.0145\): The slope is significant, meaning periodontitis status has a statistically significant effect on the probability of using dental floss.

#### 4. Summary of Results
**Individuals without periodontitis**:
   - The probability of using dental floss is approximately **22%**.
**Individuals with periodontitis**:
   - They are **52.5% as likely** to use dental floss compared to those without periodontitis.
**The effect of periodontitis on floss use is statistically significant**:
   - \(p = 0.0145\): Indicates a significant relationship between periodontitis status and floss usage behavior.
**Unexpected finding**:
   - Contrary to expectations, individuals with periodontitis are less likely to use dental floss. This contradicts common assumptions (e.g., individuals with gum disease might be more likely to adopt dental care habits) and warrants further investigation into behavioral patterns or data collection factors.

## Task3
This question asks us to:
1. Show how the parameter estimates (\(\beta_1\) and \(\gamma_1\)) from both models (from Questions 1 and 2) can be derived using the actual numbers in the contingency table.
2. Prove that \(\beta_1 = \gamma_1\).

---

### Step 1: Recap of the Models

#### **Model 1** (Question 1):
\[
\text{logit}(p_x) = \beta_0 + \beta_1 x
\]
Where:
- \(x = 1\) (regular dental floss use), \(x = 0\) (no dental floss use).
- \(p_x = P(\text{Periodontitis} \mid x)\).

#### **Model 2** (Question 2):
\[
\text{logit}(p_y) = \gamma_0 + \gamma_1 y
\]
Where:
- \(y = 1\) (Periodontitis present), \(y = 0\) (No periodontitis).
- \(p_y = P(\text{Using Floss} \mid y)\).

---

### Step 2: Table Data

The contingency table provides the following counts:

| **Regularly Use of Dental Floss** | **Periodontitis (Yes)** | **Periodontitis (No)** | **Row Totals** |
|-----------------------------------|-------------------------|-------------------------|----------------|
| **Yes**                           | 22                      | 75                      | 97             |
| **No**                            | 148                     | 265                     | 413            |
| **Column Totals**                 | 170                     | 340                     | 510            |

---

### Step 3: Express \(\beta_1\) and \(\gamma_1\) Using the Table

#### **For \(\beta_1\) (Model 1)**:
The logistic regression for Model 1 calculates the odds ratio of periodontitis (\(P(\text{Periodontitis} \mid \text{Floss})\)) for floss users versus non-users. From the table:
- Odds of periodontitis for floss users:
  \[
  \text{Odds}_\text{Yes} = \frac{22}{75}
  \]
- Odds of periodontitis for non-floss users:
  \[
  \text{Odds}_\text{No} = \frac{148}{265}
  \]
- Odds ratio (OR):
  \[
  \text{OR} = \frac{\text{Odds}_\text{Yes}}{\text{Odds}_\text{No}} = \frac{\frac{22}{75}}{\frac{148}{265}} = \frac{22 \cdot 265}{75 \cdot 148}
  \]
- Log odds ratio (\(\beta_1\)):
  \[
  \beta_1 = \log(\text{OR}) = \log\left(\frac{22 \cdot 265}{75 \cdot 148}\right)
  \]

#### **For \(\gamma_1\) (Model 2)**:
The logistic regression for Model 2 calculates the odds ratio of floss use (\(P(\text{Floss} \mid \text{Periodontitis})\)) for individuals with periodontitis versus those without. From the table:
- Odds of floss use for individuals with periodontitis:
  \[
  \text{Odds}_\text{Yes} = \frac{22}{148}
  \]
- Odds of floss use for individuals without periodontitis:
  \[
  \text{Odds}_\text{No} = \frac{75}{265}
  \]
- Odds ratio (OR):
  \[
  \text{OR} = \frac{\text{Odds}_\text{Yes}}{\text{Odds}_\text{No}} = \frac{\frac{22}{148}}{\frac{75}{265}} = \frac{22 \cdot 265}{148 \cdot 75}
  \]
- Log odds ratio (\(\gamma_1\)):
  \[
  \gamma_1 = \log(\text{OR}) = \log\left(\frac{22 \cdot 265}{75 \cdot 148}\right)
  \]

---

### Step 4: Prove \(\beta_1 = \gamma_1\)

By the definitions above, both \(\beta_1\) and \(\gamma_1\) are calculated as:
\[
\log\left(\frac{22 \cdot 265}{75 \cdot 148}\right)
\]
Thus, we have:
\[
\beta_1 = \gamma_1
\]

This equality occurs because the odds ratio for Model 1 (Periodontitis as a function of floss use) and Model 2 (Floss use as a function of periodontitis) share the same underlying data structure and proportion relationships in the contingency table. The log odds ratio remains invariant regardless of which variable is treated as the dependent variable.

---

### Step 5: Conclusion

The parameter estimates (\(\beta_1\) from Model 1 and \(\gamma_1\) from Model 2) are equal because they are both derived from the same odds ratio in the contingency table. This demonstrates the symmetry of logistic regression in cases like this, where the relationship between two binary variables is analyzed.

# Exercise2:2
In an experiment,165 mice were exposed to various doses of bensoapyren (BaP)over a ten month period.It was then examined how many of the mice that has developed a lung tumor.The result as a function of logarithmic dose is:
```{r}
#Input data and create a data frame
logdos<-c(-7.60,-6.22,-4.60,-3.00,-1.39,0.92)
n<-c(18,19,28,32,28,40)
x<-c(1,2,4,9,12,32)
data22 <- data.frame(logdos,x,n)
data22
```
## Task1
Estimate for each dose separately the risk (probability)to develop a lung tumor. Make a plot of the risk against log(dos). Calculate the logodds for tumor and plot it against log(dose).Seems the logistic regression model appropriate for these data?


The risk (or probability) of developing a lung tumor is calculated as:
\[
\text{Risk} = \frac{\text{Tumor (x)}}{\text{Total mice (n)}}
\]

The log-odds (logits) are calculated as:
\[
\text{Log-Odds} = \log\left(\frac{\text{Risk}}{1 - \text{Risk}}\right)
\]
```{r}
# Calculate risk (probability)
data22$risk <- data22$x / data22$n
# Calculate log-odds
data22$log_odds <- log(data22$risk / (1 - data22$risk))
print(data22)
```

```{r}
# Plot risk against log(dose)
plot(data22$logdos, data22$risk, type = "b", pch = 19, xlab = "Log(Dose)", ylab = "Risk", main = "Risk vs Log(Dose)")

```
```{r}
# Plot log-odds against log(dose)
plot(data22$logdos, data22$log_odds, type = "b", pch = 19, xlab = "Log(Dose)", ylab = "Log-Odds", main = "Log-Odds vs Log(Dose)")

```
Seems the logistic regression model appropriate for these data?

**Data Characteristics**:  
   Logistic regression is suitable for binary data. In this case, the response variable is the probability of tumor occurrence, which meets the conditions for applying a logistic regression model.
**Linear Trend of Log-Odds**:  
   From the plotted relationship between Log-Odds and Log(Dose), a linear trend is observed, it further supports the applicability of the logistic regression model.

## Task2
Fit a logistic regression model to this data and interpret the parameter estimates, particularly the slope parameter.
```{r}
# Fit a logistic regression model
model <- glm(cbind(x, n - x) ~ logdos, data = data22, family = binomial(link = logit))

# Model summary
summary(model)
```
```{r}
# Add fitted values to the plot of log-odds
plot(data22$logdos, data22$log_odds, type = "b", pch = 19, xlab = "Log(Dose)", ylab = "Log-Odds", main = "Log-Odds vs Log(Dose)")
lines(data22$logdos, predict(model, type = "link"), col = "red")
```
### 1. **Graph Observation: Log-Odds vs Log(Dose)**

- The black dots represent the calculated log-odds for each dose, while the red curve shows the fitted logistic regression model.
- **Linear Trend**: The relationship between Log-Odds and Log(Dose) appears approximately linear, and the red fitted curve aligns closely with the black dots. This indicates that the assumption of linearity between Log-Odds and Log(Dose) holds, making the logistic regression model appropriate for the data.

### 2. **Model Output Interpretation**

#### **Coefficients**
- **Intercept (\( \beta_0 = 0.687 \))**:
  - When Log(Dose) is 0 (i.e., the dose is 1), the log-odds of developing a tumor is 0.687.
  - Converting to probability:
    \[
    P = \frac{\exp(0.687)}{1 + \exp(0.687)} \approx 0.665
    \]
    **Interpretation**: At a dose of 1, the probability of developing a tumor is approximately 66.5%.

- **Slope (\( \beta_1 = 0.520 \))**:
  - For every one-unit increase in Log(Dose), the log-odds of developing a tumor increases by 0.520.
  - Converting to odds ratio (OR):
    \[
    \text{OR} = \exp(0.520) \approx 1.682
    \]
    **Interpretation**: For every one-unit increase in Log(Dose), the odds of developing a tumor increase by approximately 68.2%.

#### **Significance**
- The p-value for the intercept is \(p = 0.00764\), and for the slope, \(p < 0.001\), indicating that both are statistically significant. This shows that Log(Dose) has a significant effect on tumor development.

### 3. **Model Fit**
- **Residual Deviance (1.2416)**:
  - The residual deviance is much smaller than the null deviance (56.5321), indicating that the model fits the data well.
- **Degrees of Freedom**:
  - The residual degrees of freedom (4) are consistent with the complexity of the data.
- **AIC (24.024)**:
  - A low AIC value indicates that the model is both parsimonious and provides a good fit to the data.

### 4. **Conclusion**

1. **Suitability of the Logistic Regression Model**:
   - The relationship between Log-Odds and Log(Dose) is linear.
   - The residual deviance is low, showing a good fit, and all model parameters are statistically significant.

2. **Relationship Between Dose and Tumor Development**:
   - Higher doses (Log(Dose)) significantly increase the probability of tumor development (\(\beta_1 > 0\)).

## Task3
Find the covariance matrix for the estimates and assess if they are correlated.Find a 95 confidence interval for the parameters.Find a 95 confidence interval for the tumor risk at dose 0.25 (log(dose)=-1.39).
```{r}
vcov(model)
confint.default(model,level=0.95)
corr <- vcov(model)[1, 2] / sqrt(vcov(model)[1, 1] * vcov(model)[2, 2])
```
**Covariance** and **correlation**:
   - The covariance between `(Intercept)` and `logdos` is `0.0144`, indicating a small linear relationship between the two parameters. This suggests limited correlation between the intercept and the dose effect.
**Significance**:
   - From the confidence intervals, the interval for `logdos` does not include 0, confirming that \(\log(\text{dose})\) has a significant impact on tumor development.
**Effect of Dose**:
   - The coefficient for `logdos` lies within [0.3537, 0.6870], indicating that increasing the dose significantly increases the log-odds of tumor occurrence.

## Task4
Perform a Wald-test of \( H_0 : \beta = 0 \). Explain and show with an own calculation, based on R-output, how the test statistic is constructed.

The Wald test statistic is calculated as:
\[
W = \frac{\hat{\beta}_1^2}{\text{Var}(\hat{\beta}_1)}
\]
Where:
- \(\hat{\beta}_1\): Estimated regression coefficient for \( \log(\text{dose}) \).
- \(\text{Var}(\hat{\beta}_1)\): Variance of the estimated coefficient (obtained from the diagonal of the covariance matrix).
```{r}
# Extract the coefficient and standard error for log(dose)
beta1 <- coef(model)["logdos"]  # Extract the coefficient for log(dose)
se_beta1 <- sqrt(vcov(model)["logdos", "logdos"])  # Extract the variance and compute standard error

# Calculate the Wald statistic
wald_stat <- (beta1^2) / (se_beta1^2)

# Display the Wald statistic
cat("Wald Statistic:", wald_stat, "\n")

# Calculate the p-value (chi-square distribution with 1 degree of freedom)
p_value <- 1 - pchisq(wald_stat, df = 1)

# Display the p-value
cat("P-value:", p_value, "\n")

# Draw conclusions based on the p-value
if (p_value < 0.05) {
  cat("Reject the null hypothesis: beta1 is significantly different from 0.\n")
} else {
  cat("Fail to reject the null hypothesis: beta1 is not significantly different from 0.\n")
}
```
**Conclusion**: The dose (log-transformed) has a significant positive effect on tumor development probability.

## Task5
Q:The variance of a parameter estimate is generally inversely proportional to the sample size.Is this true also here?Since we have no expression explicitly for the variances, we cannot confirm it straightforward.In order to investigate it,you should multiply all counts in the table by 10 repeatedly (10,100,1000)and report what you observe.
```{r}
# Define the original dataset
logdos <- c(-7.60, -6.22, -4.60, -3.00, -1.39, 0.92)
n <- c(18, 19, 28, 32, 28, 40)
x <- c(1, 2, 4, 9, 12, 32)

# Function to scale counts and fit logistic regression
investigate_variance <- function(scale_factor) {
  # Scale the counts
  n_scaled <- n * scale_factor
  x_scaled <- x * scale_factor
  
  # Create a new dataset
  data_scaled <- data.frame(logdos, x = x_scaled, n = n_scaled)
  
  # Fit the logistic regression model
  model_scaled <- glm(cbind(x, n - x) ~ logdos, data = data_scaled, family = binomial(link = logit))
  
  # Extract the variance of beta1 (logdos)
  beta1_var <- vcov(model_scaled)["logdos", "logdos"]
  
  # Return results
  return(list(scale_factor = scale_factor, beta1_var = beta1_var))
}

# Perform the investigation for scale factors 10, 100, 1000
results <- lapply(c(10, 100, 1000), investigate_variance)

# Display the results
results_df <- do.call(rbind, lapply(results, as.data.frame))
colnames(results_df) <- c("Scale Factor", "Variance of Beta1")
print(results_df)
```
#### Observations:
- As the scale factor (sample size) increases, the variance of \(\beta_1\) decreases significantly, confirming the inverse relationship.
- For example:
  - At a scale factor of 10: Variance is relatively large.
  - At a scale factor of 100: Variance is 1/10th of the previous case.
  - At a scale factor of 1000: Variance is further reduced to 1/100th of the original.

#### Conclusion:
This empirical investigation confirms that the variance of a parameter estimate in logistic regression is inversely proportional to the sample size. This is consistent with statistical theory.
