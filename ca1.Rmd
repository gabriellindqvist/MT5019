---
title: "Computer Assignment 1"
author: "Gabriel Lindqvist, Jiahui Li"
date: "`r Sys.Date()`"
output: pdf_document
---
# Exercise1.1
```{r, echo=FALSE}
# Input the data
survey_data <- matrix(c(309, 191, 319, 281), nrow = 2, byrow = TRUE)
rownames(survey_data) <- c("Women", "Men")
colnames(survey_data) <- c("In favor", "Against")
survey_data

women_in_favor <- survey_data[1,1]
men_in_favor <- survey_data[1,2]
women_total <- sum(survey_data[1, ])
men_total <- sum(survey_data[2, ])

# Calculate percentages for women
women_percent_in_favor <- (women_in_favor / women_total) * 100
women_percent_against <- 100 - women_percent_in_favor

# Calculate percentages for men
men_percent_in_favor <- (men_in_favor / men_total) * 100
men_percent_against <- 100 - men_percent_in_favor

# Print results
cat("Women: In favor =", women_percent_in_favor, "%, Against =", women_percent_against, "%\n")
cat("Men: In favor =", men_percent_in_favor, "%, Against =", men_percent_against, "%\n")

```
# Exercise1.2
```{r, echo = FALSE}
# Calculate Pearson's Chi-squared test
chi_squared_result <- chisq.test(survey_data, correct = FALSE)
chi_squared_result
# Calculate expected counts
expected_counts <- chi_squared_result$expected
expected_counts

# Calculate likelihood ratio statistic G^2
observed <- as.vector(survey_data)
expected <- as.vector(expected_counts)
G2 <- 2 * sum(observed * log(observed / expected))
G2

# Print results
cat("Pearson's Chi-squared statistic (X^2):", chi_squared_result$statistic, "\n")
cat("Likelihood ratio statistic (G^2):", G2, "\n")
cat("P-value (from Chi-squared test):", chi_squared_result$p.value, "\n")

# Conclusion
if (chi_squared_result$p.value < 0.05) {
  cat("Conclusion: Reject the null hypothesis. There is a significant association between gender and opinion on legal abortion.\n")
} else {
  cat("Conclusion: Fail to reject the null hypothesis. There is no significant association between gender and opinion on legal abortion.\n")
}

```
# Exercise 1.3
```{r, echo=FALSE}
# Calculate odds for women and men
odds_women <- survey_data[1, 1] / survey_data[1, 2]
odds_men <- survey_data[2, 1] / survey_data[2, 2]

# Odds ratio
odds_ratio <- odds_women / odds_men

# Log odds ratio and its standard error
log_odds_ratio <- log(odds_ratio)
se_log_odds_ratio <- sqrt(1 / survey_data[1, 1] + 1 / survey_data[1, 2] +
                          1 / survey_data[2, 1] + 1 / survey_data[2, 2])

# 95% confidence interval for the log odds ratio
ci_log_lower <- log_odds_ratio - 1.96 * se_log_odds_ratio
ci_log_upper <- log_odds_ratio + 1.96 * se_log_odds_ratio

# Transform back to get the confidence interval for the odds ratio
ci_lower <- exp(ci_log_lower)
ci_upper <- exp(ci_log_upper)

# Print results
cat("Odds for women (In favor / Against):", odds_women, "\n")
cat("Odds for men (In favor / Against):", odds_men, "\n")
cat("Odds Ratio (Women vs Men):", odds_ratio, "\n")
cat("95% Confidence Interval for Odds Ratio: [", ci_lower, ",", ci_upper, "]\n")

# Interpretation
if (1 < ci_lower || 1 > ci_upper) {
  cat("Interpretation: The odds of being in favor of legal abortion significantly differ between women and men.\n")
} else {
  cat("Interpretation: The odds of being in favor of legal abortion do not significantly differ between women and men.\n")
}
```

1. **Odds Calculation**: The odds for women and men are calculated as \( \text{In favor} / \text{Against} \) for each group.
2. **Odds Ratio**: The odds ratio is the ratio of the odds for women to the odds for men.
3. **Confidence Interval**: 
   - The log of the odds ratio is used to calculate the confidence interval, as the log odds follow a normal distribution.
   - The standard error of the log odds ratio is computed using the formula \( \sqrt{1/a + 1/b + 1/c + 1/d} \), where \( a, b, c, d \) are the cell counts.
   - The confidence interval is transformed back to the odds ratio scale.
4. **Interpretation**: Based on the confidence interval, it is concluded whether the odds differ significantly between the two groups.

# Exercise1.4
```{r,echo=FALSE}
# Calculate risks (probabilities of being in favor)
risk_women <- survey_data[1, 1] / women_total
risk_men <- survey_data[2, 1] / men_total

# Risk ratio
risk_ratio <- risk_women / risk_men

# Log risk ratio and its standard error
log_risk_ratio <- log(risk_ratio)
se_log_risk_ratio <- sqrt((1 / survey_data[1, 1]) - (1 / women_total) +
                          (1 / survey_data[2, 1]) - (1 / men_total))

# 95% confidence interval for the log risk ratio
ci_log_lower <- log_risk_ratio - 1.96 * se_log_risk_ratio
ci_log_upper <- log_risk_ratio + 1.96 * se_log_risk_ratio

# Transform back to get the confidence interval for the risk ratio
ci_lower <- exp(ci_log_lower)
ci_upper <- exp(ci_log_upper)

# Print results
cat("Risk for women (In favor / Total):", risk_women, "\n")
cat("Risk for men (In favor / Total):", risk_men, "\n")
cat("Risk Ratio (Women vs Men):", risk_ratio, "\n")
cat("95% Confidence Interval for Risk Ratio: [", ci_lower, ",", ci_upper, "]\n")

# Interpretation
if (1 < ci_lower || 1 > ci_upper) {
  cat("Interpretation: The relative risk of being in favor of legal abortion significantly differs between women and men.\n")
} else {
  cat("Interpretation: The relative risk of being in favor of legal abortion does not significantly differ between women and men.\n")
}
```
### Why Do the Results Differ?
- **Odds Ratio vs Risk Ratio**:
  - Odds Ratio compares the relative odds (favor vs oppose), focusing on the ratio of proportions within each group.
  - Risk Ratio compares the direct probabilities (favor/total), reflecting absolute differences between groups.

- **Sensitivity to Data Range**:
  - When the probabilities (risks) are above 50%, the odds tend to exaggerate differences compared to the risk. In this case, women (61.8%) and men (67.6%) have relatively high probabilities, making OR and RR diverge.

---
### Conclusion:
- If the research question focuses on **relative tendencies or odds**, **Odds Ratio** is appropriate, suggesting that women are more inclined to favor legal abortion.
- If the focus is on **absolute differences in proportions**, **Risk Ratio** is more intuitive, showing no significant difference between women and men.

# Exercise 1:2

## Task 1

First we enter the given data in R and generate a contingency table.

```{r, echo = FALSE}
tab2 <- as.table(rbind(c(557, 1835 - 557), c(1198, 2691 - 1198)))
dimnames(tab2) <- list(gender = c("women", "men"), admission = c("admitted", "not admitted"))
addmargins(tab2)
addmargins(prop.table(tab2, 1), 2)
```

Next, we want to do make the same analysis as in Exercise 1:1. Firstly we can test whether there is an association between gender and admission using $\chi^2$ and $G^2$ statistics.

```{r, echo = FALSE}
library(MASS)

# Calculate chi-sq statistic, LR statistic and p-values
loglm(~gender + admission, tab2)
```

From the output above we can see that the test statistics are large with small p-values (less than 0.01). This provides strong evidence against the null hypothesis of independence, indicating a significant association between gender and admission.

Next, we calculate the odds ratio of admission for men relative relative to women.

```{r, echo = FALSE}
library(epitools)

# Calculate the odds ratio
oddsratio(tab2, method = "wald", rev = "col")$measure
```

From the output above we can see that the estimated odds ratio is $\text{OR} = 1.841$ with a $95\%$ C.I as $(1.624, 2.087)$. This suggests that that men have higher odds of being admitted compared to women. Furthermore, the CI does not include 1, which supports that the difference is statistically significant.

Lastly, we compute the risk ratio for men relative to women.

```{r, echo = FALSE}
# Calculate the risk ratio
riskratio(tab2, rev = "col")$measure
```

From the output above we can see that the estimate risk ratio $\text{RR} = 1.467$ with a $95\%$ C.I as $(1.352, 1.591)$. This indicates that men are $46.7\%$ more likely of being admitted compared to women. The C.I does not include 1 here, which further supports that there is a distinct difference in admission relative to gender.

## Task 2
The next task is to investigate the effect of replacing all values in the contingency table with one-tenth of the original values, and then comment on what we observe.

```{r, echo = FALSE}
tab2_2 <- round(tab2 / 10)
addmargins(tab2_2)
addmargins(prop.table(tab2_2, 1), 2)

loglm(~gender + admission, tab2_2)

oddsratio(tab2_2, method = "wald", rev = "col")$measure

riskratio(tab2_2, method = "wald", rev = "col")$measure
```

First we observe that the Pearson's and LR statistics values change by $1/10$, which yields higher p-values. Secondly we observe that the estimates for the odds and risk ratios are still the same. However, the $95\%$ C.I are much wider compared to using the original data. This is expected, because smaller sample sizes introduces greater uncertainty, ultimately reducing the precision of the estimates.

Next, we repeat by replacing the numbers with one hundredth of the original values.

```{r, echo = FALSE}
tab2_2 <- round(tab2_2 / 10)
addmargins(tab2_2)
addmargins(prop.table(tab2_2, 1), 2)

loglm(~gender + admission, tab2_2)

oddsratio(tab2_2, method = "wald", rev = "col")$measure

riskratio(tab2_2, method = "wald", rev = "col")$measure
```

The estimated odds and risk ratios are roughly the same as the original ones, which is expected since we just scale the data entries with $1/100$ and then round the values. Further, the statistics have also been scaled with approximately $1/100$ compared to the statistics in Task 1, which is why the p-values are now larger. Lastly the $95\%$ C.I are much wider now and does also include 1 in them (which is expected with smaller sample sizes).

## Task 3
In this task we want to create a new two-way contingency table that satisfy the following conditions:

- The sample odds ratio $\hat{\theta}$ is within the interval $(0.99, 1.01)$.

- The population odds ratio $\theta$ is significantly different from 1.

Finally, we will comment on the relevance of declaring 'statistical significance' in a situation the one like above.

### Creating the Contingency Table

In order for $\hat{\theta}$ to satisfy the first condition, we know that each cell count $n_{ij}, \ i,j = 1, 2$ needs to be approximately the same such that

$$
\hat{\theta} = \frac{n_{11} \cdot n_{22}}{n_{12} \cdot n_{21}} \approx 1.
$$

To satisfy the second condition, we need larger $n_{ij}$, as larger sample sizes increase statistical power, making small imbalances more likely to yield significant results. With the reasoning above, we construct the following contingency table

```{r, echo = FALSE}
tab3 <- as.table(rbind(c(200000, 199000), c(199000, 200000)))
dimnames(tab3) <- list(group = c("Group 1", "Group 2"), outcome = c("Outcome 1", "Outcome 2"))

oddsratio(tab3, method = "wald")$data
oddsratio(tab3, method = "wald")$measure
```

From above, we observe that the sample odds ratio satisfies the first condition, with $\hat{\theta} \approx 1.01$. Next, we test whether the second condition holds.

```{r, echo = FALSE}
loglm(~group + outcome, tab3)
```

The test output shows that the null hypothesis of independence is rejected at the $5\%$ significance level. Thus satisfying the second condition.

### Comments on Above Results
The results above highlights the fact that statistical tests are sensitive to small imbalances when sample sizes are large, even if the sample odds ratio is weak. In the case above, the sample odds ratio indicates that there is no association, yet the large sample size leads to a significant test. This demonstrates that statistical significance does not always equate to practical relevance, which is why relying solely on statistical significance can be misleading in cases like the one above. Instead, both the sample odds ratio and the tests should be included when reporting the results, whilst weighing the relevance of the test.
